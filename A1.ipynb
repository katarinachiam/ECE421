{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "A1_submit.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3F2xEXOQXGo_"
      },
      "source": [
        "# Assignment : Logistic Regression\n",
        "\n",
        "Katarina Chiam (1004908996)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "M38ru8HVXaOj",
        "outputId": "aa62abfc-3298-40c3-a562-a3177cd514de"
      },
      "source": [
        "#google drive mount to colab statement\n",
        "import os\n",
        "from google.colab import drive\n",
        "\n",
        "drive.mount('/content/drive')\n",
        "root = '/content/drive/My Drive/_Y3/ECE421_W/A1'\n",
        "os.chdir(root)"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eNRpaNpuXEeC"
      },
      "source": [
        "%tensorflow_version 1.x\n",
        "#starter code given below\n",
        "\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def loadData():\n",
        "    with np.load('notMNIST.npz') as dataset:\n",
        "        Data, Target = dataset['images'], dataset['labels']\n",
        "        posClass = 2\n",
        "        negClass = 9\n",
        "        dataIndx = (Target==posClass) + (Target==negClass)\n",
        "        Data = Data[dataIndx]/255.\n",
        "        Target = Target[dataIndx].reshape(-1, 1)\n",
        "        Target[Target==posClass] = 1\n",
        "        Target[Target==negClass] = 0\n",
        "        np.random.seed(421)\n",
        "        randIndx = np.arange(len(Data))\n",
        "        np.random.shuffle(randIndx)\n",
        "        Data, Target = Data[randIndx], Target[randIndx]\n",
        "        trainData, trainTarget = Data[:3500], Target[:3500]\n",
        "        validData, validTarget = Data[3500:3600], Target[3500:3600]\n",
        "        testData, testTarget = Data[3600:], Target[3600:]\n",
        "    return trainData, validData, testData, trainTarget, validTarget, testTarget"
      ],
      "execution_count": 63,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KJ9XyIA3ZFKz"
      },
      "source": [
        "def loss(W, b, x, y, reg):\n",
        "    # Your implementation here\n",
        "    N = (x.shape[0])\n",
        "    W_re = W.reshape((x.shape[1] * x.shape[2], 1))\n",
        "    x_re = x.reshape((N, x.shape[1] * x.shape[2]))\n",
        "    y_hat = sigmoid(np.matmul(x_re, W_re) + b)\n",
        "    CE = np.sum(np.multiply(-1, y)*np.log(y_hat) - (1-y)*np.log(1-y_hat))\n",
        "    reg_loss = reg/2*np.sum(W_re*W_re)\n",
        "    return (1/N*CE + reg_loss)\n",
        "\n",
        "def sigmoid(z):\n",
        "  return (1/(1+np.e**(-z)))"
      ],
      "execution_count": 64,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lK24qoIdb49N"
      },
      "source": [
        "def grad_loss(W, b, x, y, reg):\n",
        "  # Your implementation here\n",
        "    N = (x.shape[0])\n",
        "    W_re = W.reshape((x.shape[1] * x.shape[2], 1))\n",
        "    x_re = x.reshape((N, x.shape[1] * x.shape[2]))\n",
        "    y_hat = sigmoid(np.matmul(x_re, W_re) + b)\n",
        "    grad_W = x_re * (y_hat-y) + reg*W_re.transpose()\n",
        "    grad_b = y_hat-y\n",
        "    return np.mean(grad_W, axis = 0), np.mean(grad_b)"
      ],
      "execution_count": 65,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HD4DVBmsAPem"
      },
      "source": [
        "def acc(W, b, x, y):\n",
        "  W_re = W.reshape((x.shape[1] * x.shape[2], 1))\n",
        "  x_re = x.reshape((x.shape[0], x.shape[1] * x.shape[2]))\n",
        "  prediction = np.matmul(x_re, W_re) + b\n",
        "  act = sigmoid(prediction)\n",
        "  decision = np.where(act>0.5, 1, 0)\n",
        "  correct = (decision == y).sum()\n",
        "  return correct/y.shape[0]"
      ],
      "execution_count": 66,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ai55VPH7wSr3"
      },
      "source": [
        "def grad_descent(W, b, x, y, alpha, epochs, reg, error_tol):\n",
        "    # Your implementation here\n",
        "    trainData, validData, testData, trainTarget, validTarget, testTarget = loadData()\n",
        "    trainLoss = []\n",
        "    validLoss = []\n",
        "    testLoss = []\n",
        "    trainAcc = []\n",
        "    validAcc = []\n",
        "    testAcc = []\n",
        "    W_new = W\n",
        "    b_new = b\n",
        "    for i in range(epochs):\n",
        "      trainLoss.append(loss(W_new, b_new, trainData, trainTarget, reg))\n",
        "      validLoss.append(loss(W_new, b_new, validData, validTarget, reg))\n",
        "      testLoss.append(loss(W_new, b_new, testData, testTarget, reg))\n",
        "      grad_W, grad_b = grad_loss(W_new, b_new, trainData, trainTarget, reg)\n",
        "      trainAcc.append(acc(W_new, b_new, trainData, trainTarget))\n",
        "      validAcc.append(acc(W_new, b_new, validData, validTarget))\n",
        "      testAcc.append(acc(W_new, b_new, testData, testTarget))\n",
        "      W_old_norm = np.linalg.norm(W_new)\n",
        "      W_new = W_new - alpha * grad_W\n",
        "      W_new_norm = np.linalg.norm(W_new)\n",
        "      b_new = b_new - alpha * grad_b\n",
        "      if np.abs(W_old_norm - W_new_norm) < error_tol:\n",
        "        break\n",
        "    plot(trainLoss, validLoss, 1, reg) #note: just for plotting purposes\n",
        "    plot(trainAcc, validAcc, 2, reg) #note: see plot function Appendix A1\n",
        "    return (W_new, b_new)\n",
        "\n",
        "def plot(train, valid, fignum, reg):\n",
        "  plt.figure(fignum)\n",
        "  plot1, = plt.plot(train)\n",
        "  plot2, = plt.plot(valid)\n",
        "  plt.xlabel('Epochs')\n",
        "  if fignum == 1:\n",
        "    plt.ylabel('Loss')\n",
        "    plt.title('Loss vs. Epochs with Learning Rate = 0.005 and λ = {:1.3f}'.format(reg))\n",
        "  elif fignum == 2:\n",
        "    plt.ylabel('Accuracy')\n",
        "    plt.title('Accuracy vs. Epochs with Learning Rate = 0.005 and λ = {:1.3f}'.format(reg))\n",
        "  plt.legend([plot1,plot2], ['Training', 'Validation'])\n",
        "  plt.show()\n",
        "  return True"
      ],
      "execution_count": 68,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PyeEp6eOEv50"
      },
      "source": [
        "np.random.seed(421)\n",
        "\n",
        "W = np.random.normal(loc = 0, scale = 0.5, size = 784)\n",
        "b = 0\n",
        "\n",
        "trainData, validData, testData, trainTarget, validTarget, testTarget = loadData()\n",
        "\n",
        "x = trainData\n",
        "y = trainTarget\n",
        "alpha = 0.005\n",
        "epochs = 5000\n",
        "reg = 0\n",
        "error_tol = 10^-7\n",
        "\n",
        "grad_descent(W, b, x, y, alpha, epochs, reg, error_tol)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "10GLfgnM6z3g"
      },
      "source": [
        "# Part 2: Logistic Regression in Tensorflow"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LnuESWKwclUU"
      },
      "source": [
        "def buildGraph(alpha = 0.001, beta_1 = None, beta_2 = None, e = None, batch_size = 500, epochs = 700, reg = 0):\n",
        "  #build computation graph \n",
        "\n",
        "  #weights and bias\n",
        "  W = tf.Variable(tf.truncated_normal(shape=(784, 1), mean=0.0, stddev=0.5))\n",
        "  bias = tf.Variable(0.00)\n",
        "\n",
        "  #variables for input - data, labels and reg\n",
        "  x = tf.placeholder(tf.float32, shape = (batch_size, 784), name = 'x')\n",
        "  y = tf.placeholder(tf.float32, shape = (batch_size, 1), name = 'y')\n",
        "  reg_val = tf.placeholder(tf.float32, name = 'reg_val')\n",
        "\n",
        "  valid_x = tf.placeholder(tf.float32, shape=(100, 784), name = 'valid_x')\n",
        "  valid_y = tf.placeholder(tf.float32, shape=(100, 1), name = 'valid_y')\n",
        "\n",
        "  test_x = tf.placeholder(tf.float32, shape=(145, 784), name = 'test_x')\n",
        "  test_y = tf.placeholder(tf.float32, shape=(145, 1), name = 'test_y')\n",
        "\n",
        "  samples = {\"tr\": x, \"v\":valid_x, \"te\":test_x}\n",
        "  labels = {\"tr\":y, \"v\":valid_y, \"te\":test_y}\n",
        "\n",
        "  params = {\"b1\":beta_1, \"b2\":beta_2, \"epsilon\":e, \"bs\":batch_size, \"epoch\":epochs, \"regular\":reg}\n",
        "\n",
        "  #loss\n",
        "  y_hat, loss = tf_loss(W, bias, x, y, reg)\n",
        "  v_y_hat, v_loss = tf_loss(W, bias, valid_x, valid_y, reg_val)\n",
        "  t_y_hat, t_loss = tf_loss(W, bias, test_x, test_y, reg_val)\n",
        "\n",
        "  #optimizer\n",
        "  optimizer = tf.train.AdamOptimizer(learning_rate=alpha).minimize(loss) #batch\n",
        "  #optimizer = tf.train.AdamOptimizer(learning_rate=alpha, beta1 = beta_1, beta2 = beta_2, epsilon = e).minimize(loss) #hyperparameter\n",
        "\n",
        "  #implement SGD\n",
        "  SGD(optimizer, W, bias, loss, v_loss, t_loss, samples, labels, reg, reg_val, params)\n",
        "  return W, bias, y_hat, y, loss, optimizer"
      ],
      "execution_count": 57,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wXUpnA8Ihn4S"
      },
      "source": [
        "def SGD(optimizer, W, bias, loss, v_loss, t_loss, samples, labels, reg, reg_val, params):\n",
        "  trainLoss = []\n",
        "  validLoss = []\n",
        "  testLoss = []\n",
        "  trainAcc = []\n",
        "  validAcc = []\n",
        "  testAcc = []\n",
        "\n",
        "  with tf.Session() as sess:\n",
        "    init_op = tf.global_variables_initializer()\n",
        "    sess.run(init_op)\n",
        "\n",
        "    trainData, validData, testData, trainTarget, validTarget, testTarget = loadData()\n",
        "    trainData = trainData.reshape(trainData.shape[0], trainData.shape[1]*trainData.shape[2])\n",
        "    validData = validData.reshape(validData.shape[0], validData.shape[1]*validData.shape[2])\n",
        "    testData = testData.reshape(testData.shape[0], testData.shape[1]*testData.shape[2])\n",
        "\n",
        "    batch_size = params[\"bs\"]\n",
        "    for ep in range(params[\"epoch\"]):\n",
        "      batch_num = int(trainData.shape[0]/batch_size)\n",
        "      sample_index = np.random.choice(trainData.shape[0], size = trainData.shape[0], replace=False)\n",
        "      trainData_sample = trainData[sample_index]\n",
        "      trainTarget_sample = trainTarget[sample_index]\n",
        "      trLossaccum =0\n",
        "      vLossaccum =0\n",
        "      teLossaccum =0\n",
        "      for b in range(batch_num):\n",
        "        x_re = np.reshape(trainData_sample[b*batch_size:(b+1)*batch_size], (batch_size, 784))\n",
        "        y_re = np.reshape(trainTarget_sample[b*batch_size:(b+1)*batch_size], (batch_size, 1))\n",
        "        feed = {samples[\"tr\"]:x_re,\n",
        "                labels[\"tr\"]:y_re,\n",
        "                samples[\"v\"]:validData,\n",
        "                labels['v']:validTarget,\n",
        "                samples['te']:testData,\n",
        "                labels['te']:testTarget,\n",
        "                reg_val: reg}\n",
        "        _, update_W, update_b, trLoss, vLoss, teLoss = sess.run([optimizer, W, bias, loss, v_loss, t_loss], feed_dict = feed)\n",
        "        trLossaccum += np.mean(trLoss)\n",
        "        vLossaccum += np.mean(vLoss)\n",
        "        teLossaccum += np.mean(teLoss)\n",
        "      trainLoss.append(trLossaccum/batch_num)\n",
        "      validLoss.append(vLossaccum/batch_num)\n",
        "      testLoss.append(teLossaccum/batch_num)\n",
        "      trainAcc.append(tf_acc(update_W, update_b, trainData, trainTarget))\n",
        "      validAcc.append(tf_acc(update_W, update_b, validData, validTarget))\n",
        "      testAcc.append(tf_acc(update_W, update_b, testData, testTarget))\n",
        "  print('train acc:', trainAcc[-1])\n",
        "  print('valid acc:', validAcc[-1])\n",
        "  print('test acc:', testAcc[-1])\n",
        "  #tf_plot(trainLoss, validLoss, 1, params[\"b1\"], params[\"b2\"], params[\"epsilon\"]) #see Appendix A2 for plotting code\n",
        "  #tf_plot(trainAcc, validAcc, 2, params[\"b1\"], params[\"b2\"], params[\"epsilon\"])\n",
        "  return True"
      ],
      "execution_count": 59,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wvQw_p1Nc2XW"
      },
      "source": [
        "beta_one = [0.95, 0.99]\n",
        "beta_two = [0.99, 0.999]\n",
        "e_list = [1e-9, 1e-4]\n",
        "for o in beta_one:\n",
        "  for t in beta_two:\n",
        "    for ep in e_list:\n",
        "      print('β1 = {0}, β2 = {1}, ϵ = {2:.1E}'.format(o, t, ep))\n",
        "      buildGraph(beta_1 = o, beta_2 = t, e = ep)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DMDQ2EhociWd"
      },
      "source": [
        "def tf_plot(train, valid, fignum, beta_1, beta_2, e):\n",
        "  plt.figure(fignum)\n",
        "  plot1, = plt.plot(train)\n",
        "  plot2, = plt.plot(valid)\n",
        "  plt.xlabel('Epochs')\n",
        "  if fignum == 1:\n",
        "    plt.ylabel('Loss')\n",
        "    plt.title(r'Loss vs. Epochs with Learning Rate = 0.001, λ = 0, Batch Size = 500,' + '\\n' +  'β1 = {0}, β2 = {1}, ϵ = {2:.1E}'.format(beta_1, beta_2, e), wrap = True)\n",
        "  elif fignum == 2:\n",
        "    plt.ylabel('Accuracy')\n",
        "    plt.title(r'Accuracy vs. Epochs with Learning Rate = 0.001, λ = 0, Batch Size = 500,'  + '\\n' +  'β1 = {0}, β2 = {1}, ϵ = {2:.1E}'.format(beta_1, beta_2, e), wrap = True)\n",
        "  plt.legend([plot1,plot2], ['Training', 'Validation'])\n",
        "  plt.show()\n",
        "  return True"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "brJnyIfYce45"
      },
      "source": [
        "def tf_loss(W, b, x, y, reg):\n",
        "  y_hat = tf.sigmoid(tf.matmul(x, W) + b)\n",
        "  CE_loss = tf.nn.sigmoid_cross_entropy_with_logits(labels=y, logits=tf.matmul(x, W) + b)\n",
        "  reg_loss = tf.matmul(tf.transpose(W), W) * 0.5 * reg\n",
        "  loss = CE_loss + reg_loss\n",
        "  return y_hat, loss"
      ],
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nP1HfY7ocaPw"
      },
      "source": [
        "def tf_acc(W, b, x, y):\n",
        "  prediction = np.matmul(x, W) + b\n",
        "  act = sigmoid(prediction)\n",
        "  decision = np.where(act>0.5, 1, 0)\n",
        "  correct = (decision == y).sum()\n",
        "  return correct/y.shape[0]"
      ],
      "execution_count": 6,
      "outputs": []
    }
  ]
}